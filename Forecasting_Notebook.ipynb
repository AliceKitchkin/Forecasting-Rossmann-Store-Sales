{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliceKitchkin/Forecasting-Rossmann-Store-Sales/blob/main/Forecasting_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN5iqQON-Cqm"
      },
      "source": [
        "# Vorhersagen von Rossmann Store Sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inhaltsverzeichnis <a id=\"0\"></a> <br>\n",
        "1. [Einleitung](#1)\n",
        "2. [Package- und Datenimport](#2)  \n",
        "3. [Deskriptive Analyse](#3)  \n",
        "    3.1 [Datenüberblick](#3.1)   \n",
        "    3.2 [Datentypen](#3.2)  \n",
        "    3.3 [Betrachtung der Verteilung](#3.3)   \n",
        "    3.4 [Analyse der Kategorischen Variablen](#3.4)  \n",
        "    3.5 [Fehlende Werte](#3.5)   \n",
        "    3.6 [Ausreißer](#3.6)  \n",
        "    3.7 [Zeitreihenanalyse](#3.7)   \n",
        "    3.8 [Korrelationen](#3.8)  \n",
        "4. [Daten anpassen](#4)  \n",
        "5. [Geeignete Merkmale](#5)  \n",
        "6. [Vergleichsmetrik](#6)\n",
        "7. [ML Verfahren 1](#7)\n",
        "8. [ML Verfahren 2](#8)\n",
        "9. [Vergleich](#9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Einleitung <a id=\"1\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aFYOXeVj-kB"
      },
      "source": [
        "Dieses Jupyter Notebook dokumentiert unser Projekt für das Modul Data Mining, in dem wir die Verkaufszahlen der Rossmann-Filialen vorhersagen. Diese Aufgabe basiert auf dem Rossmann Store Sales Datensatz von [Kaggle.com](https://www.kaggle.com/competitions/rossmann-store-sales/overview), der umfangreiche Verkaufsdaten von über 1000 Filialen enthält. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBWUyUN2pCXj"
      },
      "source": [
        "## 2. Package- und Datenimport <a id=\"2\"></a> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H5IYtG_pwKa"
      },
      "outputs": [],
      "source": [
        "import calendar\n",
        "import locale\n",
        "import zipfile\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "from matplotlib.pyplot import rcParams\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "%pip install pmdarima -q\n",
        "import pmdarima as pm\n",
        "\n",
        "# Machine Learning Verfahren Nummer 2 - Prophet\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation\n",
        "# from dask.distributed import Client\n",
        "import holidays\n",
        "\n",
        "sns.set_style(style='white') # Hintergrund der Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "OwrlQxWBpkeB",
        "outputId": "d82e8dcb-5870-411d-9191-30c2df29e2bc"
      },
      "outputs": [],
      "source": [
        "# unzip train-file, to large for github\n",
        "# train.csv is included in .gitignore\n",
        "with zipfile.ZipFile(\"./data/train.zip\", \"r\") as zip:\n",
        "    zip.extract('train.csv', \"./data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Maximale Anzahl an Spalten und Zeilen, beim anzeigen von Dataframes\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yq9tBkF_kT0"
      },
      "source": [
        "## 3. Deskriptive Analyse <a id=\"3\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Datensatzüberblick <a id=\"3.1\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Der Rossmann Store Sales-Datensatz enthält historische Verkaufsdaten für 1.115 Rossmann-Filialen. Er besteht aus drei CSV-Dateien:\n",
        "\n",
        "- train.csv: Historische Daten einschließlich Verkäufe\n",
        "- test.csv: Historische Daten ohne Verkäufe (für die Vorhersage)\n",
        "- store.csv: Zusätzliche Informationen über die Filialen\n",
        "\n",
        "Im ersten Schritt werden die Spalten der Datensätze umbenannt und in Variablen gespeichert.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spalten umbennen, damit alles einheitlich auf deutsch ist\n",
        "train_original = pd.read_csv(\"./data/train.csv\").rename(columns={\"Store\":\"Filiale\",\n",
        "                                                                 \"DayOfWeek\":\"Wochentag\",\n",
        "                                                                 \"Date\":\"Datum\",\n",
        "                                                                 \"Sales\":\"Umsatz\",\n",
        "                                                                 \"Customers\":\"Kundenanzahl\",\n",
        "                                                                 \"Open\":\"Geoeffnet\",\n",
        "                                                                 \"Promo\":\"Aktionstag\",\n",
        "                                                                 \"StateHoliday\":\"Feiertag\",\n",
        "                                                                 \"SchoolHoliday\":\"Schulferien\"})\n",
        "\n",
        "test_original = pd.read_csv(\"./data/test.csv\").rename(columns={\"Id\":\"ID\",\n",
        "                                                                  \"Store\":\"Filiale\",\n",
        "                                                                  \"DayOfWeek\":\"Wochentag\",\n",
        "                                                                  \"Date\":\"Datum\",\n",
        "                                                                  \"Open\":\"Geoeffnet\",\n",
        "                                                                  \"Promo\":\"Aktionstag\",\n",
        "                                                                  \"StateHoliday\":\"Feiertag\",\n",
        "                                                                  \"SchoolHoliday\":\"Schulferien\"})\n",
        "\n",
        "store_original = pd.read_csv(\"./data/store.csv\").rename(columns={\"Store\":\"Filiale\",\n",
        "                                                                  \"StoreType\":\"Filialentyp\",\n",
        "                                                                  \"Assortment\":\"Sortiment\",\n",
        "                                                                  \"CompetitionDistance\":\"Wettbewerberentfernung\",\n",
        "                                                                  \"CompetitionOpenSinceMonth\":\"Wettbewerber_Eroeffnet_seit_Monat\",\n",
        "                                                                  \"CompetitionOpenSinceYear\":\"Wettbewerber_Eroeffnet_seit_Jahr\",\n",
        "                                                                  \"Promo2\": \"Teilnahme_Langzeitaktion\",\n",
        "                                                                  \"Promo2SinceWeek\":\"Aktion_seit_Woche\",\n",
        "                                                                  \"Promo2SinceYear\":\"Aktion_seit_Jahr\",\n",
        "                                                                  \"PromoInterval\":\"Aktionsmonate\"})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Um in einem späteren Zeitpunkt auf die originalen Datensätze zugreifen zu können, werden diese hier separat gespeichert.\n",
        "# Außerdem werden die Datensätze _train_original_ und _store_original_ über die Spalte _Filiale_ verbunden und im neuen Datensatz \n",
        "# _train_x_store_ gespeichert. Für den Datensatz _test_original_ wird das gleiche Prozedere angewandt.\n",
        "train = train_original\n",
        "test = test_original\n",
        "store = store_original\n",
        "\n",
        "train_x_store = pd.merge(train, store)\n",
        "test_x_store = pd.merge(test, store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Spalte                        | Umbenennung                           | Beschreibung                                                         |\n",
        "|-------------------------------|---------------------------------------|----------------------------------------------------------------------|\n",
        "| `Id`                          | `ID`                                  | Eindeutige ID für jede Filiale und jedes Datumspaar im Testdatensatz.|\n",
        "| `Store`                       | `Filiale`                             | Eindeutige ID für jede Filiale.|\n",
        "| `Sales`                       | `Wochentag`                           | Wochentag als Zahl von 1 (Montag) bis 7 (Sonntag) |\n",
        "| `Date`                        | `Datum`                               | Datum (im \"yyyy-mm-dd\" Format) |\n",
        "| `Sales`                       | `Umsatz`                              | Umsatz an einem bestimmten Tag (zu prognostizierendes Ziel).|\n",
        "| `Customers`                   | `Kundenanzahl`                        | Anzahl der Kunden an einem bestimmten Tag.|\n",
        "| `Open`                        | `Geoeffnet`                           | Indikator, ob die Filiale geöffnet war (0 = geschlossen, 1 = geöffnet).|\n",
        "| `Promo`                       | `Aktionstag`                          | Gibt an, ob ein Geschäft an diesem Tag eine Werbeaktion durchführt.|\n",
        "| `StateHoliday`                | `Feiertag`                            | Feiertagstyp (a = öffentlicher Feiertag, b = Osterfeiertag, c = Weihnachten, 0 = keiner).|\n",
        "| `SchoolHoliday`               | `Schulferien`                         | Gibt an, ob die Filiale von Schulschließungen betroffen war.|\n",
        "| `StoreType`                   | `Filialtyp`                           | Unterscheidet zwischen 4 verschiedenen Filialmodellen (a, b, c, d).|\n",
        "| `Assortment`                  | `Sortiment`                           | Beschreibt das Sortiment (a = grundlegend, b = extra, c = erweitert).|\n",
        "| `CompetitionDistance`         | `Wettbewerberentfernung`              | Entfernung in Metern zum nächsten Wettbewerbergeschäft.|\n",
        "| `CompetitionOpenSinceMonth`   | `Wettbewerber_Eroeffnet_seit_Monat`   | Gibt den Monat an, in dem der nächste Wettbewerber eröffnet wurde.|\n",
        "| `CompetitionOpenSinceYear`    | `Wettbewerber_Eroeffnet_seit_Jahr`    | Gibt das Jahr an, in dem der nächste Wettbewerber eröffnet wurde.|\n",
        "| `Promo2`                      | `Teilnahme_Langzeitaktion`            | Promo2 ist eine fortlaufende und aufeinanderfolgende Werbeaktion für einige Geschäfte (0 = nein, 1 = ja).|\n",
        "| `Promo2SinceWeek`             | `Aktion_seit_Woche`                   | Beschreibt die Kalenderwoche, in der das Geschäft an Promo2 teilnimmt.|\n",
        "| `Promo2SinceYear`             | `Aktion_seit_Jahr`                    | Beschreibt das Jahr, in der das Geschäft an Promo2 teilnimmt.|\n",
        "| `PromoInterval`               | `Aktionsmonate`                       | Beschreibt die aufeinanderfolgenden Intervalle, in denen Promo2 gestartet wird (z. B. \"Feb, Mai, Aug, Nov\").|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Datensätze: Train, Test, Store, Train_x_store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(store.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(train_x_store.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Datentypen <a id=\"3.2\"></a> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datentypen vor Korrektur\n",
        "train_x_store.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anpassungen Train\n",
        "train_x_store[\"Datum\"] = pd.to_datetime(train_x_store[\"Datum\"])\n",
        "train_x_store[\"Wochentag\"] = train_x_store[\"Datum\"].dt.weekday\n",
        "\n",
        "# Anpassungen Test\n",
        "test_x_store[\"Datum\"] = pd.to_datetime(test_x_store[\"Datum\"])\n",
        "test_x_store[\"Wochentag\"] = test_x_store[\"Datum\"].dt.weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datentypen nach Korrektur\n",
        "display(train_x_store.dtypes)\n",
        "\n",
        "display(test_x_store.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Neue Spalten <a id=\"3.2\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nach der Datentypenkorrektur werden folgende neuen Spalten aus den bereits existierenden hinzugefügt:\n",
        "| Originale Spalte              | Neue Spalte                           | Beschreibung                                                              |\n",
        "|-------------------------------|---------------------------------------|---------------------------------------------------------------------------|\n",
        "| `Datum`                       | `Tag`                                 | Gibt den Tag aus der ursprünglichen Spalte Datum an (von 1 bis 31).       |\n",
        "| `Datum`                       | `Monat`                               | Gibt den Monat aus der ursprünglichen Spalte Datum an (von 1 bis 12).     |\n",
        "| `Datum`                       | `Jahr`                                | Gibt das Jahr aus der ursprünglichen Spalte Datum an.             \t    |\n",
        "| `Datum`                       | `Quartal`                             | Gibt das Quartal aus der ursprünglichen Spalte Datum an (von 1 bis 4).    |\n",
        "| `Umsatz und Kundenanzahl`     | `UmsatzProKunde`                      | Gibt prozentual den Umsatz je Kunde pro Tag an.                           |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neue_spalten = {\n",
        "    \"Tag\": train_x_store[\"Datum\"].dt.day,\n",
        "    \"Monat\": train_x_store[\"Datum\"].dt.month,\n",
        "    \"Jahr\": train_x_store[\"Datum\"].dt.year,\n",
        "    \"Quartal\": train_x_store[\"Datum\"].dt.quarter\n",
        "}\n",
        "\n",
        "# Dataframe teilen, um die neuen Spalten in gewünschter Reihenfolge einzufügen\n",
        "train_x_store_before = train_x_store.iloc[:, :1]\n",
        "train_x_store_after = train_x_store.iloc[:, 1:]\n",
        "\n",
        "# Alten Dataframe mit den neuen Spalten zusammenführen\n",
        "train_x_store = pd.concat([train_x_store_before, pd.DataFrame(neue_spalten), train_x_store_after], axis=1)\n",
        "\n",
        "display(train_x_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neue_spalten = { \"UmsatzProKunde\":  train['Umsatz']/train['Kundenanzahl'] }\n",
        "\n",
        "# Dataframe teilen, um die neuen Spalten in gewünschter Reihenfolge einzufügen\n",
        "train_x_store_before = train_x_store.iloc[:, :9]\n",
        "train_x_store_after = train_x_store.iloc[:, 9:]\n",
        "\n",
        "# Alten Dataframe mit den neuen Spalten zusammenführen\n",
        "train_x_store = pd.concat([train_x_store_before, pd.DataFrame(neue_spalten), train_x_store_after], axis=1)\n",
        "\n",
        "display(train_x_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Betrachtung der Verteilung <a id=\"3.3\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es werden die statistischen Kennzahlen für den train_x_store-Datensatz berechnet, auf drei Dezimalstellen gerundet und die Datentypen angezeigt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "describe = round(train_x_store.describe(include='all'), 3)\n",
        "dtypes = pd.DataFrame(train_x_store.dtypes, columns=[\"dtypes\"]).T\n",
        "display(pd.concat([dtypes, describe]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Analyse der kategorischen Variablen <a id=\"3.4\"></a> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Betrachtung der Filialtypen und der Verteilung der Filialen auf diese Typen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filialentyp = pd.DataFrame(data = store[\"Filialentyp\"].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "df_filialentyp[\"Anteil\"] = round(df_filialentyp[\"Anzahl\"] / df_filialentyp[\"Anzahl\"].sum() * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "df_filialentyp.sort_index(inplace = True)\n",
        "display(df_filialentyp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verteilung der Kundenzahlen und Umsätze auf die Filialtypen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filialentyp_x = train_x_store.groupby('Filialentyp')[['Kundenanzahl', 'Umsatz']].sum()\n",
        "\n",
        "df_filialentyp_x = pd.merge(df_filialentyp, df_filialentyp_x, on =\"Filialentyp\")\n",
        "\n",
        "df_filialentyp_x[\"Kundenzahl_pro_Filiale\"] = round(df_filialentyp_x[\"Kundenanzahl\"] / df_filialentyp_x[\"Anzahl\"], 0).astype(int)\n",
        "df_filialentyp_x[\"Umsatz_pro_Filiale\"] = round(df_filialentyp_x[\"Umsatz\"] / df_filialentyp_x[\"Anzahl\"], 0).astype(int)\n",
        "\n",
        "# Reihenfolge der Spalten ändern\n",
        "df_filialentyp_x = df_filialentyp_x[[\"Anzahl\", \"Anteil\", \"Kundenanzahl\", \"Kundenzahl_pro_Filiale\", \"Umsatz\", \"Umsatz_pro_Filiale\"]]\n",
        "\n",
        "display(df_filialentyp_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Betrachtung der Sortimente und der Verteilung der Sortimente auf die Filialen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sortimenttyp = pd.DataFrame(data = store[\"Sortiment\"].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "df_sortimenttyp[\"Anteil\"] = round(df_sortimenttyp[\"Anzahl\"] / df_sortimenttyp[\"Anzahl\"].sum() * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "df_sortimenttyp.sort_index(inplace = True)\n",
        "display(df_sortimenttyp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Betrachtung der Sortimente und der Verteilung der Sortimente auf die Filialen unter Berücksichtigung der Filialtypen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filialentyp_x_sortimenttyp = pd.DataFrame(data = store[[\"Filialentyp\", \"Sortiment\"]].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "\n",
        "df_filialentyp_x_sortimenttyp[\"Gruppenanteil\"] = round(df_filialentyp_x_sortimenttyp[\"Anzahl\"] / df_filialentyp_x_sortimenttyp.groupby(level = \"Filialentyp\")[\"Anzahl\"].transform(\"sum\") * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "df_filialentyp_x_sortimenttyp[\"Gesamtanteil\"] = round(df_filialentyp_x_sortimenttyp[\"Anzahl\"] / df_filialentyp_x_sortimenttyp[\"Anzahl\"].sum() * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "df_filialentyp_x_sortimenttyp.sort_index(inplace = True)\n",
        "display(df_filialentyp_x_sortimenttyp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Von den Filialen des Typs a haben 381 das Sortiment a, damit haben 63,29% der Filialen des Typs a das Sortiment a, was 34,17% aller Filialtyps und Sortimentskombinationen ausmacht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Betrachtung der Angabe Feiertag (Häufigkeit der einzelnen Ausprägungen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feiertag = pd.DataFrame(data= train_x_store[\"Feiertag\"].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "display(df_feiertag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aus irgendeinem Grund taucht die 0 (kein Feiertag) zwei mal als Ausprägungstyp auf, daher untersuchen wir den Datentyp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index in df_feiertag.index:\n",
        "    print(f\"{index}: {type(index)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Die Kategorie 0 (kein Feiertag) ist einmal als str und einmal als int codiert. Das passen wir im folgenden an."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_store[\"Feiertag\"] = train_x_store[\"Feiertag\"].astype(str)\n",
        "\n",
        "df_feiertag = pd.DataFrame(data= train_x_store[\"Feiertag\"].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "display(df_feiertag)\n",
        "\n",
        "for index in df_feiertag.index:\n",
        "    print(f\"{index}: {type(index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feiertag[\"Anteil\"] = round(df_feiertag[\"Anzahl\"] / df_feiertag[\"Anzahl\"].sum() * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "display(df_feiertag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "97% der Datenpunkte haben für das Merkmal Feiertag die Ausprägung 0 (kein Feiertag). Damit machen die Feiertage rund 3% der in den Daten enthaltenen Tage aus. Das entspricht dem Erwartungswert, da in Deutschland die Zahl der gesetzlichen Feiertage, je nach Bundesland zwischen 10 und 12 liegt (pro Jahr) --> [11 / 365 = 0,03]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Geöffnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_geoeffnet = pd.DataFrame(data= train_x_store[\"Geoeffnet\"].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "df_geoeffnet[\"Anteil\"] = round(df_geoeffnet[\"Anzahl\"] / df_geoeffnet[\"Anzahl\"].sum() * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "display(df_geoeffnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Von den Datenpunkten im Datensatz train_x_store haben 83% die Ausprägung \"Geöffnet\" = 1 und 17% die Ausprägung \"Geoeffnet\" = 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Ferien oder an Feiertagen geöffnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_geoeffnet = pd.DataFrame(data= train_x_store[[\"Schulferien\", \"Feiertag\", \"Geoeffnet\"]].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "df_geoeffnet[\"Anteil\"] = round(df_geoeffnet[\"Anzahl\"] / df_geoeffnet[\"Anzahl\"].sum() * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "display(df_geoeffnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "66,88% der Datenpunkte im Datensatz train_x_store besitzen die Ausprägungskombination (Geoffnet, keine Ferien, kein Feiertag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Schulferien und Feiertag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ferien_x_feiertag = pd.DataFrame(data= train_x_store[[\"Schulferien\", \"Feiertag\"]].value_counts()).rename(columns= {\"count\": \"Anzahl\"})\n",
        "df_ferien_x_feiertag[\"Anteil\"] = round(df_ferien_x_feiertag[\"Anzahl\"] / df_ferien_x_feiertag[\"Anzahl\"].sum() * 100, 2).apply(lambda x: f\"{x}%\")\n",
        "display(df_ferien_x_feiertag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 Fehlende Werte<a id=\"3.5\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Überblick über den zeitlichen Verlauf (liegen für jeden Tag Angaben vor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anzahl Tage im Datensatz\n",
        "number_of_days = len(pd.unique(train_x_store[\"Datum\"]))\n",
        "\n",
        "# Anzahl an Tagen zwischen der ersten und letzten Datumsangabe\n",
        "first_day= train_x_store[\"Datum\"].min()\n",
        "last_day = train_x_store[\"Datum\"].max()\n",
        "expected_number_of_days = (last_day - first_day).days + 1\n",
        "\n",
        "print(f\"Anzahl an Tagen im Datensatz: {number_of_days}\\n\"\n",
        "      + f\"Differenz zwischen kleinster und größter Datumsangabe (in Tagen): {expected_number_of_days-1}\\n\"\n",
        "      + f\"Erwarte Anzahl an Tagen im Datensatz: {expected_number_of_days}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Die Anzahl der Tage für den Datenpunkte im Datensatz vorhanden sind deckt sich mit der Zeitspanne zwischen der ersten und letzten Datumsangabe im Datensatz. Es gibt also keinen Tag an dem uns alle Werte fehlen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Überischt über die Filialen:\n",
        "- liegen im Datensatz für alle Filialen und für jeden Tag Werte vor?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anzahl an Filian\n",
        "number_of_stores = len(pd.unique(train_x_store[\"Filiale\"]))\n",
        "\n",
        "stores_with_missing_data = train_x_store[\"Filiale\"].value_counts()\n",
        "stores_with_missing_data = stores_with_missing_data[stores_with_missing_data != number_of_days]\n",
        "\n",
        "number_of_stores_with_missing_data = len(stores_with_missing_data)\n",
        "\n",
        "print(f\"Anzahl Filialen bei denen Daten fehlen: {number_of_stores_with_missing_data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Für insgesamt 181 Filialen sind die Daten unvollständig. Der Grund dafür könnte sein, dass die Filialen im Laufe des betrachteten Zeitraus eröffnet oder geschlossen haben. Dafür betrachten wir im folgenden den zeitlichen Verlauf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_store_gp_date = train_x_store.groupby(\"Datum\")\n",
        "\n",
        "number_of_stores_by_date = {}\n",
        "\n",
        "for date, df in train_x_store_gp_date:\n",
        "    number_of_stores_by_date[date] = len(pd.unique(df[\"Filiale\"]))\n",
        " \n",
        "df_number_of_stores_by_date = pd.DataFrame.from_dict(number_of_stores_by_date, orient = \"index\", columns= [\"Number_of_Stores\"])\n",
        "\n",
        "fig, ax = plt.subplots(figsize= (20,6))\n",
        "\n",
        "ax.set_title(\"Anzahl der Filialen für die pro Tag ein Datensatz vorliegt\")\n",
        "ax.plot(df_number_of_stores_by_date.index, df_number_of_stores_by_date[\"Number_of_Stores\"])\n",
        "ax.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Über einen Zeitraum von ca. 6 Monaten fehlen die Daten für 181 Filialen. Da der Zeitraum für den dieses Datenpunkte fehlen, innerhalb des Betrachtungshorzionts und nicht etwa am Rande liegen, ist die Theorie der Neueröffnung oder dauerhaften Schließung hinfällig."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aber wie viele Daten fehlen insgesamt?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_datapoints = len(train_x_store)\n",
        "expected_number_of_datapoint = number_of_stores * number_of_days\n",
        "\n",
        "number_of_missing_datapoints = expected_number_of_datapoint - number_of_datapoints\n",
        "\n",
        "number_of_datapoints, expected_number_of_datapoint, number_of_missing_datapoints\n",
        "print(f\"Anzahl erhaltener Datenpunkte: {number_of_datapoints}\\n\"\n",
        "      + f\"Anzahl erwarteter Datenpunkte: {expected_number_of_datapoint-1}\\n\"\n",
        "      + f\"Anzahl fehlender Datenpunkte: {number_of_missing_datapoints} ({round(number_of_missing_datapoints / expected_number_of_datapoint * 100, 2)}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Insgesamt fehlen 33121 Datenpunkte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fehlende Werte in Train_x_store\n",
        "missing_data_abs = train_x_store.isnull().sum()\n",
        "missing_data_per = ( round(train_x_store.isnull().sum()/train_x_store.shape[0]*100, 2)).apply(lambda x: f\"{x}%\")\n",
        "\n",
        "missing_data_table = pd.DataFrame({\n",
        "    \"Fehlende Werte (absolut)\": missing_data_abs,\n",
        "    \"Fehlende Werte (prozentual)\": missing_data_per\n",
        "})\n",
        "\n",
        "display(missing_data_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fehlende Werte im Train-Store-Datensatz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Spalte: Wettbewerberentfernung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Im Datensatz gibt es einige Spalten mit fehlenden Werten, welche wir uns nach und nach ansehen werden. Wir beginnen mit der Spalte _Wettbewerberentfernung_, worin 2.642 NaN-Werte gefunden wurden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_store[pd.isnull(train_x_store.Wettbewerberentfernung)].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "describe = pd.DataFrame(round(train_x_store[\"Wettbewerberentfernung\"].describe(), 3))\n",
        "display(describe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(train_x_store[\"Wettbewerberentfernung\"], bins=50, edgecolor='white')\n",
        "plt.title('Verteilung der Wettbewerberentfernungen')\n",
        "plt.xlabel('Wettbewerberentfernung')\n",
        "plt.ylabel('Anzahl')\n",
        "\n",
        "median = train_x_store[\"Wettbewerberentfernung\"].median()\n",
        "mean = train_x_store[\"Wettbewerberentfernung\"].mean()\n",
        "\n",
        "plt.axvline(median, color='red', linestyle='dashed', linewidth=1, label=f'Median: {median:.2f}')\n",
        "plt.axvline(mean, color='green', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wir nehmen  an, dass die Daten einfach nicht vorhanden sind und ersetzen daher die fehlenden Werte in _Wettbewerberentfernung_ mit dem Median.\n",
        "Warum Median?\n",
        "-  Robustheit gegenüber Ausreißern: Der Median ist weniger anfällig für Ausreißer im Vergleich zum arithmetischen Mittelwert\n",
        "- Verteilung der Daten: Wenn die Verteilung der Daten nicht symmetrisch ist oder nicht normal verteilt ist, kann der Median oft die zentrale Tendenz der Daten besser widerspiegeln als der Mittelwert. Dies ist häufig der Fall bei Daten, die rechtsschief oder linksschief verteilt sind.\n",
        "- Der Median ist ein besserer Indikator für die zentrale Tendenz der Daten, insbesondere wenn man bedenkt, dass die Standardabweichung (7.706,913) relativ hoch ist. Dies deutet darauf hin, dass die Daten möglicherweise eine gewisse Varianz oder Ausreißer aufweisen könnten, die den Mittelwert verzerren würden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_store[\"Wettbewerberentfernung\"].fillna(train_x_store[\"Wettbewerberentfernung\"].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Spalten: _Wettbewerber_Eroeffnet_seit_Monat_, _Wettbewerber_Eroeffnet_seit_Jahr_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(train_x_store[(train_x_store.Wettbewerberentfernung>0) & (pd.isnull(train_x_store.Wettbewerber_Eroeffnet_seit_Monat))].head())\n",
        "display(train_x_store[(train_x_store.Wettbewerberentfernung>0) & (pd.isnull(train_x_store.Wettbewerber_Eroeffnet_seit_Jahr))].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Da zwischen den Spalten Wettbewerberentfernung, _Wettbewerber_Eroeffnet_seit_Monat_ und _Wettbewerber_Eroeffnet_seit_Jahr_ kaum eine Korrelation (Korrelationskoeffizienten liegen nah um 0 herum) besteht und die Anzahl an fehlenden Werten (32%) zu groß ist um sie zu entfernen, werden die fehlenden Werte durch den jewieligen Median ersetzt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "median_W_Eroeffnet_seit_Monat = train_x_store['Wettbewerber_Eroeffnet_seit_Monat'].median()\n",
        "median_W_Eroeffnet_seit_Jahr = train_x_store['Wettbewerber_Eroeffnet_seit_Jahr'].median()\n",
        "\n",
        "train_x_store['Wettbewerber_Eroeffnet_seit_Monat'].fillna(median_W_Eroeffnet_seit_Monat, inplace=True)\n",
        "train_x_store['Wettbewerber_Eroeffnet_seit_Jahr'].fillna(median_W_Eroeffnet_seit_Jahr, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Spalten: _Teilnahme_Langzeitaktion_, _Aktion_seit_Woche_, _Aktion_seit_Jahr_, _Aktionsmonate_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Als nächstes sehen wir uns die Spalten _Teilnahme_Langzeitaktion_, _Aktion_seit_Woche_ und _Aktion_seit_Jahr_ an. Sofern eine Filiale an einer Langzeitaktion teilnimmt, sollten auch Monat und Jahr nicht fehlen. Da dies der Fall ist, können wir die fehlenden Werte durch Null ersetzen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(train_x_store[(train_x_store.Teilnahme_Langzeitaktion==1) & (pd.isnull(train_x_store.Aktion_seit_Woche))])\n",
        "display(train_x_store[(train_x_store.Teilnahme_Langzeitaktion==1) & (pd.isnull(train_x_store.Aktion_seit_Jahr))])\n",
        "display(train_x_store[(train_x_store.Teilnahme_Langzeitaktion==1) & (pd.isnull(train_x_store.Aktionsmonate))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_store[\"Aktion_seit_Woche\"].fillna(0, inplace=True)\n",
        "train_x_store[\"Aktion_seit_Jahr\"].fillna(0, inplace=True)\n",
        "train_x_store[\"Aktionsmonate\"].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fehlende Werte im Test-Store-Datensatz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fehlende Werte in Test_x_store\n",
        "missing_data_abs = test_x_store.isnull().sum()\n",
        "missing_data_per = ( round(test_x_store.isnull().sum()/test_x_store.shape[0]*100, 2)).apply(lambda x: f\"{x}%\")\n",
        "\n",
        "missing_data_table = pd.DataFrame({\n",
        "    \"Fehlende Werte (absolut)\": missing_data_abs,\n",
        "    \"Fehlende Werte (prozentual)\": missing_data_per\n",
        "})\n",
        "\n",
        "display(missing_data_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Spalte: Geoffnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_x_store.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nur Filiale 622 hat NaNs in der Spalte Geoffnet\n",
        "display(test_x_store[pd.isnull(test_x_store.Geoeffnet)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datum extrahieren, wo Geoffnet fehlende Werte hat\n",
        "unique_dates = test_x_store[pd.isnull(test_x_store['Geoeffnet'])]['Datum'].unique()\n",
        "\n",
        "# Datensatz nach dem Datum filtern\n",
        "filtered_df = test_x_store[test_x_store['Datum'].isin(unique_dates)]\n",
        "\n",
        "# Nach Datum und Geoffnet gruppieren\n",
        "grouped = filtered_df.groupby(['Datum', 'Geoeffnet']).size().unstack(fill_value=0)\n",
        "grouped.columns = ['Anzahl_Geschlossen', 'Anzahl_Geoeffnet']\n",
        "\n",
        "display(grouped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Da die aller meisten Filialen an den ausgewählten Tagen geöffnet sind, werden die fehlenden Werte in der Spalte _Geoffnet_ ebenfalls auf 1 gesetzt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_x_store[\"Geoeffnet\"].fillna(1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Restliche Spalten mit fehlenden Werten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hier gehen wir genau so vor, wie im Train-Store-Datensatz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Median des Train-Datensatzes benutzen\n",
        "test_x_store[\"Wettbewerberentfernung\"].fillna(test_x_store[\"Wettbewerberentfernung\"].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Median des Train-Datensatzes benutzen\n",
        "median_W_Eroeffnet_seit_Monat = test_x_store['Wettbewerber_Eroeffnet_seit_Monat'].median()\n",
        "median_W_Eroeffnet_seit_Jahr = test_x_store['Wettbewerber_Eroeffnet_seit_Jahr'].median()\n",
        "\n",
        "test_x_store['Wettbewerber_Eroeffnet_seit_Monat'].fillna(median_W_Eroeffnet_seit_Monat, inplace=True)\n",
        "test_x_store['Wettbewerber_Eroeffnet_seit_Jahr'].fillna(median_W_Eroeffnet_seit_Jahr, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_x_store[\"Aktion_seit_Woche\"].fillna(0, inplace=True)\n",
        "test_x_store[\"Aktion_seit_Jahr\"].fillna(0, inplace=True)\n",
        "test_x_store[\"Aktionsmonate\"].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_x_store.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prüfen, ob noch fehlende Werte vorhanden sind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_data_abs = train_x_store.isnull().sum()\n",
        "missing_data_per = ( round(train_x_store.isnull().sum()/train_x_store.shape[0]*100, 2)).apply(lambda x: f\"{x}%\")\n",
        "\n",
        "missing_data_table = pd.DataFrame({\n",
        "    \"Fehlende Werte (absolut)\": missing_data_abs,\n",
        "    \"Fehlende Werte (prozentual)\": missing_data_per\n",
        "})\n",
        "\n",
        "display(missing_data_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_data_abs = test_x_store.isnull().sum()\n",
        "missing_data_per = ( round(test_x_store.isnull().sum()/test_x_store.shape[0]*100, 2)).apply(lambda x: f\"{x}%\")\n",
        "\n",
        "missing_data_table = pd.DataFrame({\n",
        "    \"Fehlende Werte (absolut)\": missing_data_abs,\n",
        "    \"Fehlende Werte (prozentual)\": missing_data_per\n",
        "})\n",
        "\n",
        "display(missing_data_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.7 Ausreißer <a id=\"3.6\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hier betrachten wir verschiedene Umsatzszenarien, um den Datensatz besser zu verstehen und Ausreißer zu ermitteln."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kein_umsatz = train_x_store[train_x_store.Umsatz==0.0]\n",
        "print(\"An \" + str(kein_umsatz.shape[0]) + \" von \" + str(train_x_store.shape[0]) + \" Tagen (\" + str(round(kein_umsatz.shape[0]/train_x_store.shape[0]*100,2)) + \"%) wurde kein Umsatz gemacht.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kein_umsatz_und_geoeffnet = train_x_store[(train_x_store[\"Umsatz\"]==0.0) & (train_x_store[\"Geoeffnet\"]==1)]\n",
        "print(\"An \" + str(kein_umsatz_und_geoeffnet.shape[0]) + \" von \" + str(train_x_store.shape[0]) + \" Tagen (\" + str(round(kein_umsatz_und_geoeffnet.shape[0]/train_x_store.shape[0]*100,2)) + \"%) wurde kein Umsatz gemacht, obwohl diese Filialen geöffnet waren.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kein_umsatz_geoeffnet_grouped = kein_umsatz_und_geoeffnet.groupby([\"Wochentag\", \"Geoeffnet\"]).agg(\n",
        "    Anzahl_Tage=('Umsatz', 'size'),   # Anzahl der Zeilen\n",
        "    Umsatz=('Umsatz', 'sum')     # Summe der Umsätze\n",
        ")\n",
        "\n",
        "kein_umsatz_geoeffnet_grouped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Offene Filialen ohne Umsatz sind zwar selten und nicht unrealistisch, aber wären für die Vorhersage problematisch. Außerdem werden auch alle Zeilen entfernt, in denen die Filiale geschlossen war und es keinen Umsatz gab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_store = train_x_store.drop( train_x_store[(train_x_store[\"Umsatz\"]==0.0) & (train_x_store[\"Geoeffnet\"]==1)].index )\n",
        "train_x_store = train_x_store.drop( train_x_store[(train_x_store[\"Umsatz\"]==0.0) & (train_x_store[\"Geoeffnet\"]==0)].index )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Was jedoch unrealistisch wäre, sind geschlossene Filialen mit Umsatz - das prüfen wir als nächstes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "umsatz_geschlossen = train_x_store[(train_x_store[\"Umsatz\"]!= 0) & (train_x_store[\"Geoeffnet\"]==0)].shape[0]\n",
        "print(\"An \" + str(umsatz_geschlossen) + \" von \" + str(train_x_store.shape[0]) + \" Tagen (\" + str(round(umsatz_geschlossen/train_x_store.shape[0]*100,2)) + \"%) wurde Umsatz gemacht, obwohl diese Filialen geschlossen waren.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.8 Zeitreihenanalyse <a id=\"3.7\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Trendanalyse: Darstellung des Umsatzes über die Zeit, um Trends, saisonale Muster oder Zyklen zu identifizieren.\n",
        "- Saisonale Muster: Analyse von Wochen-, Monats- und Jahresmustern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot der Summe des Umsatzes und der Kundenzahl im zeitlichen Verlauf\n",
        "revenue = train_x_store.groupby(\"Datum\")[\"Umsatz\"].sum()\n",
        "customer = train_x_store.groupby(\"Datum\")[\"Kundenanzahl\"].sum()\n",
        "\n",
        "fig, axs = plt.subplots(3, figsize=(30,15))\n",
        "\n",
        "\n",
        "axs[0].plot(revenue.index, revenue.values, label = \"Umsatz\")\n",
        "axs[0].set_title(\"Summierter Umsatz\", fontsize=25, y=1)\n",
        "\n",
        "axs[1].plot(customer.index, customer.values, label = \"Kundenanzahl\", color = \"orange\")\n",
        "axs[1].set_title(\"Summierte Kundenanzahl\", fontsize=25, y=1)\n",
        "\n",
        "axs[2].plot(revenue.index, revenue.values/revenue.mean(), label = \"Umsatz (zentriert)\")\n",
        "axs[2].plot(customer.index, customer.values/customer.mean(), label = \"Kundenanzahl (zentriert)\")\n",
        "axs[2].set_title(\"Umsatz und Kundenanzahl summiert und zentriert\", fontsize=25, y=1)\n",
        "\n",
        "for ax in axs:\n",
        "    ax.grid()\n",
        "    ax.legend()\n",
        "    \n",
        "\n",
        "fig.suptitle('Summierte Umsätze und Kundenzahlen im Verlauf der Zeit', fontsize=30, y=.95)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "stores_without_missing_data = train_x_store[~train_x_store[\"Filiale\"].isin(stores_with_missing_data.index)].copy()\n",
        "\n",
        "# choose random store (id) of each store typ\n",
        "store_id_a = stores_without_missing_data[stores_without_missing_data[\"Filialentyp\"] == \"a\"][\"Filiale\"].sample(n=1).values[0]\n",
        "store_id_b = stores_without_missing_data[stores_without_missing_data[\"Filialentyp\"] == \"b\"][\"Filiale\"].sample(n=1).values[0]\n",
        "store_id_c = stores_without_missing_data[stores_without_missing_data[\"Filialentyp\"] == \"c\"][\"Filiale\"].sample(n=1).values[0]\n",
        "store_id_d = stores_without_missing_data[stores_without_missing_data[\"Filialentyp\"] == \"d\"][\"Filiale\"].sample(n=1).values[0]\n",
        "\n",
        "# store_id = 200\n",
        "\n",
        "\"\"\"\n",
        "get a sample of the training data for each choosen store id (only \"Datum\", \"Umsatz\" and \"Kundenanzahl\" is needed)\n",
        "Sample includes all data of the selected store (id)\n",
        "\"\"\"\n",
        "sample_a = train_x_store[train_x_store[\"Filiale\"]==store_id_a][[\"Datum\", \"Umsatz\", \"Kundenanzahl\"]].copy().set_index(\"Datum\", drop = True).sort_index()\n",
        "sample_b = train_x_store[train_x_store[\"Filiale\"]==store_id_b][[\"Datum\", \"Umsatz\", \"Kundenanzahl\"]].copy().set_index(\"Datum\", drop = True).sort_index()\n",
        "sample_c = train_x_store[train_x_store[\"Filiale\"]==store_id_c][[\"Datum\", \"Umsatz\", \"Kundenanzahl\"]].copy().set_index(\"Datum\", drop = True).sort_index()\n",
        "sample_d = train_x_store[train_x_store[\"Filiale\"]==store_id_d][[\"Datum\", \"Umsatz\", \"Kundenanzahl\"]].copy().set_index(\"Datum\", drop = True).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# display(sample)\n",
        "# lags = [1, 2, 3, 4, 5, 6, 7, 14, 21, 28, 30, 60, 90]\n",
        "\n",
        "# fig, axs = plt.subplots(4, figsize= (20,12))\n",
        "# plot_acf(sample_a[\"Umsatz\"], lags = 60, zero = False, ax = axs[0])\n",
        "# plot_acf(sample_b[\"Umsatz\"], lags = 60, zero = False, ax = axs[1])\n",
        "# plot_acf(sample_c[\"Umsatz\"], lags = 60, zero = False, ax = axs[2])\n",
        "# plot_acf(sample_d[\"Umsatz\"], lags = 60, zero = False, ax = axs[3])\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# figure for subplots\n",
        "plt.figure(figsize = (24, 16))\n",
        "\n",
        "# acf and pacf for sample a (Umsatz)\n",
        "plt.subplot(421); plot_acf(sample_a[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "plt.subplot(422); plot_pacf(sample_a[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "\n",
        "# acf and pacf for sample b (Umsatz)\n",
        "plt.subplot(423); plot_acf(sample_b[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "plt.subplot(424); plot_pacf(sample_b[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "\n",
        "# acf and pacf for sample c (Umsatz)\n",
        "plt.subplot(425); plot_acf(sample_c[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "plt.subplot(426); plot_pacf(sample_c[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "\n",
        "# acf and pacf for sample d (Umsatz)\n",
        "plt.subplot(427); plot_acf(sample_d[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "plt.subplot(428); plot_pacf(sample_d[\"Umsatz\"], lags = 50, ax = plt.gca(), color = \"blue\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Der Umsatz weist eine Autokorrelation zu den Vielfachen von Sieben auf, was darauf schließen lässt, dass der Umsatz vom Wochentag abhängig ist. Die Autokorrelation gibt keinen Hinweis auf einen anderen zeitlichen Einfluss. Im Folgenden wird der zeitliche Einfluss der Woche als Saisonalität aus den Daten herausgerechnet und so der Einfluss der Saisonalität dargestellt. Aufgrund der Ergebnisse der Untersuchung der Autokorrelation wird als Periode (Dauer einer Saisonalität) 7 gewählt. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_umsatz = seasonal_decompose(sample_a[\"Umsatz\"], model = \"additive\", period = 7)\n",
        "stl_yearly_umsatz = seasonal_decompose(sample_a[\"Umsatz\"], model = \"additive\", period = 365)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_umsatz.plot().suptitle('Wöchentliche Saisonalität Filialtyp A', fontsize=30, y=1.05)\n",
        "stl_yearly_umsatz.plot().suptitle('Jährliche Saisonalität Filialtyp A', fontsize=30, y=1.05)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bei der Zerlegung des Umsatzes in einen Wiederkehrenden Term (Saisonalität - Seasonal), einen den zugrunden liegenden Trend und einen unerklärlichen Teil (Residuen - Resid) ergeben sich, bei einer Periodenlänge von 7 bzw. 365 Tagen die oberen Plots. Auch wenn die Autokorrelation keinen Hinweis auf weitere zeitliche Einflüsse als die Woche (den Wochentag) gegeben hat, weist der Trend markante und wiederkehrende Erhebungen auf, die auf eine weitere Saisonalität hindeuten. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_umsatz = seasonal_decompose(sample_b[\"Umsatz\"], model = \"additive\", period = 7)\n",
        "stl_yearly_umsatz = seasonal_decompose(sample_b[\"Umsatz\"], model = \"additive\", period = 365)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_umsatz.plot().suptitle('Wöchentliche Saisonalität Filialtyp B', fontsize=30, y=1.05)\n",
        "stl_yearly_umsatz.plot().suptitle('Jährliche Saisonalität Filialtyp B', fontsize=30, y=1.05)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_umsatz = seasonal_decompose(sample_c[\"Umsatz\"], model = \"additive\", period = 7)\n",
        "stl_yearly_umsatz = seasonal_decompose(sample_c[\"Umsatz\"], model = \"additive\", period = 365)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_umsatz.plot().suptitle('Wöchentliche Saisonalität Filialtyp C', fontsize=30, y=1.05)\n",
        "stl_yearly_umsatz.plot().suptitle('Jährliche Saisonalität Filialtyp C', fontsize=30, y=1.05)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_umsatz = seasonal_decompose(sample_d[\"Umsatz\"], model = \"additive\", period = 7)\n",
        "stl_yearly_umsatz = seasonal_decompose(sample_d[\"Umsatz\"], model = \"additive\", period = 365)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_umsatz.plot().suptitle('Wöchentliche Saisonalität Filialtyp D', fontsize=30, y=1.05)\n",
        "stl_yearly_umsatz.plot().suptitle('Jährliche Saisonalität Filialtyp D', fontsize=30, y=1.05)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fig, axs = plt.subplots(3, figsize= (20,12))\n",
        "# plot_acf(sample[\"Kundenanzahl\"], lags = len(sample)-1, zero = False, ax = axs[0])\n",
        "# plot_acf(sample[\"Kundenanzahl\"], lags = 60, zero = False, ax = axs[1])\n",
        "# plot_pacf(sample[\"Kundenanzahl\"], lags = 60, zero = False, ax = axs[2])\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Die Kundenanzahl weist eine Autokorrelation zu den Vielfachen von Sieben auf, was darauf schließen lässt, dass die Kundenanzahl vom Wochentag abhängig ist. Die Autokorrelation gibt keinen Hinweis auf einen anderen zeitlichen Einfluss. Im Folgenden wird der zeitliche Einfluss der Woche als Saisonalität aus den Daten herausgerechnet und so der Einfluss der Saisonalität dargestellt. Aufgrund der Ergebnisse der Untersuchung der Autokorrelation wird als Periode (Dauer einer Saisonalität) 7 gewählt. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_kundenzahl = seasonal_decompose(sample_a[\"Kundenanzahl\"], model = \"additive\", period=7)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_kundenzahl.plot().suptitle('Wöchentliche Saisonalität Filialtyp A', fontsize=30, y=1.05)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bei der Zerlegung der Kundenanzahl in einen Wiederkehrenden Term (Saisonalität - Seasonal), einen den zugrunden liegenden Trend und einen unerklärlichen Teil (Residuen - Resid) ergeben sich, bei einer Periodenlänge von 7 bzw. 365 Tagen die oberen Plots. Auch wenn die Autokorrelation keinen Hinweis auf weitere zeitliche Einflüsse als die Woche (den Wochentag) gegeben hat, besitzt der Trend markante und wiederkehrende Erhebungen, die auf eine weitere Saisonalität hindeuten. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_kundenzahl = seasonal_decompose(sample_b[\"Kundenanzahl\"], model = \"additive\", period=7)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_kundenzahl.plot().suptitle('Wöchentliche Saisonalität Filialtyp B', fontsize=30, y=1.05)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_kundenzahl = seasonal_decompose(sample_c[\"Kundenanzahl\"], model = \"additive\", period=7)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_kundenzahl.plot().suptitle('Wöchentliche Saisonalität Filialtyp C', fontsize=30, y=1.05)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stl_weekly_kundenzahl = seasonal_decompose(sample_d[\"Kundenanzahl\"], model = \"additive\", period=7)\n",
        "\n",
        "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
        "stl_weekly_kundenzahl.plot().suptitle('Wöchentliche Saisonalität Filialtyp D', fontsize=30, y=1.05)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eine Filiale zufällig wählen\n",
        "store_id = store[\"Filiale\"].sample(n=1).values[0]\n",
        "\n",
        "# Den Datensatz auf die zuvor festgelegte Filiale filtern\n",
        "sample = train_x_store[train_x_store[\"Filiale\"]==store_id].copy()\n",
        "sample_gp_weekday = sample.groupby(by=[\"Wochentag\"])\n",
        "\n",
        "weekdays = []\n",
        "mean_revenue = []\n",
        "mean_numb_of_customers = []\n",
        "for weekday, df in sample_gp_weekday:\n",
        "    weekdays.append(weekday[0])\n",
        "    mean_revenue.append(df[\"Umsatz\"].mean())\n",
        "    mean_numb_of_customers.append(df[\"Kundenanzahl\"].mean())\n",
        "    \n",
        "\n",
        "df_avg_week = pd.DataFrame(data = {\"Wochentag\": weekdays, \"Umsatz\": mean_revenue, \"Kundenanzahl\": mean_numb_of_customers})\n",
        "display(df_avg_week)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "locale.setlocale(locale.LC_ALL, 'de_DE')\n",
        "\n",
        "fig, axs = plt.subplots(2, figsize = (10,10))\n",
        "\n",
        "axs[0].bar(calendar.day_name, df_avg_week.Umsatz, label = \"Durchchnittlicher Umsatz\")\n",
        "axs[1].bar(calendar.day_name, df_avg_week.Kundenanzahl, label = \"Durchschnittliche Kundenanzahl\")\n",
        "\n",
        "for ax in axs:\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
        "    ax.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_gp_day = sample.groupby(by=[\"Tag\"])\n",
        "\n",
        "days = []\n",
        "mean_revenue = []\n",
        "mean_numb_of_customers = []\n",
        "for day, df in sample_gp_day:\n",
        "    days.append(day[0])\n",
        "    mean_revenue.append(df[\"Umsatz\"].mean())\n",
        "    mean_numb_of_customers.append(df[\"Kundenanzahl\"].mean())\n",
        "    \n",
        "\n",
        "df_avg_day_of_month = pd.DataFrame(data = {\"Tag\": days, \"Umsatz\": mean_revenue, \"Kundenanzahl\": mean_numb_of_customers})\n",
        "display(df_avg_day_of_month.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, figsize = (20,10))\n",
        "\n",
        "axs[0].bar(df_avg_day_of_month.Tag, df_avg_day_of_month.Umsatz, label = \"Durchchnittlicher Umsatz\")\n",
        "axs[1].bar(df_avg_day_of_month.Tag, df_avg_day_of_month.Kundenanzahl, label = \"Durchschnittliche Kundenanzahl\")\n",
        "\n",
        "for ax in axs:\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
        "    ax.grid()\n",
        "\n",
        "fig.suptitle('Durchschnittliche Verteilung des Umsatzes und der Kundenzahl über einen Monat', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.9 Korrelationsanalyse <a id=\"3.8\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Korrelationsmatrix auf den gesamten Train-Store-Datensatz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Die Korrelationsmatrix beinhaltet alle numerischen Spalten aus dem _train_x_store_ Datensatz. Dabei wird bei der Pearson-Korrelation mit binären Variablen genauso wie mit kontinuierlichen Variablen umgegangen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spalten mit numerischen Werten (Strings und kategorische Werte ausgeschlossen)\n",
        "train_x_store_only_nr = train_x_store.select_dtypes(include=['number'])\n",
        "\n",
        "# Korrelationsmatrix nach Pearson-Verfahren erstellen\n",
        "corr_matrix_all = train_x_store_only_nr.corr(\"pearson\")\n",
        "display(corr_matrix_all.head())\n",
        "\n",
        "# Nur unteres Dreieck in der Korrelationsmatrix ziehen\n",
        "my_mask = np.triu(np.ones_like(corr_matrix_all, dtype=bool))\n",
        "\n",
        "# Korrelationsmatrix erstellen und formatieren\n",
        "plt.figure(figsize=(12,6))\n",
        "plot = sns.heatmap(corr_matrix_all, cmap=\"RdBu\", vmin=-1, vmax=1, annot=True, fmt=\"0.3f\", mask=my_mask)\n",
        "plot.set_title(\"Korrelationsmatrix gesamter Train-Store-Datensatz\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Korrelationsmatrix einer zufälligen Filiale des Train-Store-Datensatzes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eine Filiale zufällig wählen\n",
        "store_id = store[\"Filiale\"].sample(n=1).values[0]\n",
        "\n",
        "# Den Datensatz auf die zuvor festgelegte Filiale filtern\n",
        "sample = train_x_store[train_x_store[\"Filiale\"]==store_id].copy()\n",
        "\n",
        "# Spalten mit numerischen Werten (Strings und kategorische Werte ausgeschlossen)\n",
        "sample_only_nr = sample.select_dtypes(include=['number'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Korrelationsmatrix nach Pearson-Verfahren erstellen\n",
        "corr_matrix_sample = sample_only_nr.corr(\"pearson\")\n",
        "\n",
        "# Nur unteres Dreieck in der Korrelationsmatrix ziehen\n",
        "my_mask = np.triu(np.ones_like(corr_matrix_sample, dtype=bool))\n",
        "\n",
        "# Korrelationsmatrix erstellen und formatieren\n",
        "plt.figure(figsize=(12,6))\n",
        "plot = sns.heatmap(corr_matrix_sample, cmap=\"RdBu\", vmin=-1, vmax=1, annot=True, fmt=\"0.3f\", mask=my_mask)\n",
        "plot.set_title(f\"Korrelationsmatrix der Filiale {store_id}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Die Korrelationsmatrix weist einen großen weißen Bereich mit NaN-Werten auf, da diese Spalten konstante Werte enthalten. In solchen Fällen ist die Standardabweichung gleich null, was zur Folge hat, dass die Berechnung der Korrelation nicht möglich ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Korrelationsmatritzen miteinander vergleichen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Positive Werte zeigen an, dass der Korrelationskoeffizient in der ersten Matrix größer ist als in der zweiten Matrix.\n",
        "- Negative Werte zeigen an, dass der Korrelationskoeffizient in der zweiten Matrix größer ist als in der ersten Matrix.\n",
        "- Ein Wert von Null in der Differenzmatrix bedeutet, dass sich die Korrelationskoeffizienten zwischen den beiden Matrizen nicht unterscheiden, d.h., die Korrelationen sind für diese Variablenpaare gleich."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Differenz der beiden Korrelationsmatritzen nach Pearson-Verfahren erstellen\n",
        "diff_matrix = corr_matrix_all - corr_matrix_sample\n",
        "\n",
        "# Nur unteres Dreieck in der Korrelationsmatrix ziehen\n",
        "my_mask = np.triu(np.ones_like(diff_matrix, dtype=bool))\n",
        "\n",
        "# Korrelationsmatrix erstellen und formatieren\n",
        "plt.figure(figsize=(12,6))\n",
        "plot = sns.heatmap(diff_matrix, cmap=\"RdBu\", vmin=-1, vmax=1, annot=True, fmt=\"0.3f\", mask=my_mask)\n",
        "plot.set_title(\"Differenzmatrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqGWloyK_pKW"
      },
      "source": [
        "## Geeignete Merkmale <a id=\"4\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definieren Sie geeignete Merkmale (Features) für die Klassifikation/Regression/Clustering. Versuchen Sie dabei, aus den bestehenden Merkmale neue abzuleiten und überlegen Sie sich zusätzliche z.B. mit externen Informationen. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Umsatz je Kunde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Umsatz je Kunde\n",
        "example = train_x_store_after.copy()\n",
        "example[\"UmsatzProKunde\"] = example[\"Umsatz\"] / example[\"Kundenanzahl\"]\n",
        "\n",
        "display(example[\"UmsatzProKunde\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Wettbewerberentfernung\n",
        "\n",
        "Beispielsweise durch Bildung von Klassen [0, <250, <500 ...] oder Verwendung der nummerischen Werte (IST Zustand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Umsatz \n",
        "\n",
        "4. Kundenanzahl\n",
        "\n",
        "5. Flilialtyp\n",
        "\n",
        "6. Sortiment\n",
        "\n",
        "7. Feiertag\n",
        "\n",
        "8. Schulferien\n",
        "\n",
        "9. Datum (Wochentag, Tag im Monat, Tag im Jahr...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vergleichsmetrik <a id=\"5\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wir werden den RMSPE als Vergleichsmetrik nutzen. <br>\n",
        "\n",
        "**Vorteile des RMSPE:**\n",
        "\n",
        "* Skalenunabhängigkeit:<br>\n",
        "        Prozentuale Fehlerbewertung: RMSPE bewertet die Fehler in prozentualen Einheiten. Dies bedeutet, dass die Metrik unabhängig von der Größenordnung der Umsätze ist. Dies ist besonders nützlich, wenn Umsätze in verschiedenen Bereichen oder Zeiträumen verglichen werden sollen, da es eine einheitliche Basis für die Bewertung bietet.\n",
        "\n",
        "* Berücksichtigung großer Fehler:<br>\n",
        "        Empfindlichkeit gegenüber Ausreißern: Durch die Quadrierung der Fehler ist RMSPE besonders empfindlich gegenüber großen Fehlern. Dies ist von Vorteil, wenn große Abweichungen in den Umsatzprognosen vermieden werden sollen, da solche Abweichungen potenziell erhebliche finanzielle Auswirkungen haben können.\n",
        "\n",
        "* Vergleichbarkeit:<br>\n",
        "        Standardisierte Metrik: RMSPE ermöglicht den Vergleich der Vorhersagegenauigkeit verschiedener Modelle oder Algorithmen auf einer standardisierten Grundlage. Dies erleichtert die Auswahl des besten Modells für die Umsatzprognose.\n",
        "\n",
        "**Nachteile des RMSPE:**\n",
        "\n",
        "* Empfindlichkeit gegenüber Ausreißern:<br>\n",
        "        Überbetonung großer Fehler: Die Quadrierung der Fehler bedeutet, dass sehr große Abweichungen unverhältnismäßig stark gewichtet werden. In einigen Fällen kann dies zu einer verzerrten Einschätzung der Modellleistung führen, insbesondere wenn wenige Ausreißer die Metrik dominieren.\n",
        "\n",
        "* Komplexität der Interpretation:<br>\n",
        "        Schwierige Interpretation: Im Vergleich zu einfacheren Metriken wie dem Mean Absolute Error (MAE) kann die Interpretation der quadratischen und prozentualen Fehler für Stakeholder weniger intuitiv sein. Dies kann die Kommunikation der Modellleistung an nicht-technische Entscheidungsträger erschweren.\n",
        "\n",
        "* Probleme bei kleinen tatsächlichen Werten:<br>\n",
        "        Division durch kleine Werte: Wenn die tatsächlichen Umsatzwerte sehr klein sind, können die prozentualen Fehler und somit der RMSPE sehr groß werden. Dies kann zu instabilen und irreführenden Ergebnissen führen.\n",
        "\n",
        "Zwei der genannten Nachteile sind für uns nicht relevant: Zum einen sind unsere Stakeholder unserer Meinung nach in der Lage, die Metrik zu interpretieren, und zum anderen werden wir, wie in der ursprünglichen Challenge, Nullwerte des Umsatzes beim Scoring ignorieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validierungsdatensatz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wir benötigen zum messen und vergleichen der Performance einen Validierungsdatensatz, worin auch Umsätze enthalten sind. Den in Kaggle hinterlegten Testdatensatz benutzen wir dann für die finale Vorhersage mit den trainierten Machine Learning Modellen.\n",
        "\n",
        "Hierfür entnehmen wir dem Trainingsdatensatz einen Anteil, der genau so groß ist wie der Testdatensatz aus Kaggle in Tagen.\n",
        "\n",
        "Um den Code schlank zu halten, werden die neuen Datensätze ab hier \"train\" und validation\" genannt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datensatz nach Datum aufsteigend sortieren\n",
        "test_sortiert = test_x_store.sort_values(by=[\"Datum\"], ascending=True).reset_index(drop = True)\n",
        "\n",
        "# Differenz in Tagen\n",
        "erstes_datum = test_sortiert.iloc[1][\"Datum\"]\n",
        "letztes_datum = test_sortiert.iloc[-1][\"Datum\"]\n",
        "diff = letztes_datum - erstes_datum\n",
        "\n",
        "print(\"Der Anzahl an Tagen zwischen {0} und {1} beträgt {2}.\".format(erstes_datum, letztes_datum, diff.days))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datensatz nach Datum aufsteigend sortieren\n",
        "train_sortiert = train_x_store.sort_values(by=[\"Datum\"], ascending=True).reset_index(drop = True)\n",
        "\n",
        "# Index vom letzten Tag in test minus die Differenz\n",
        "letztes_datum = train_sortiert.iloc[-1][\"Datum\"]\n",
        "split_datum = letztes_datum - timedelta(days=diff.days)\n",
        "\n",
        "print(\"Der letzte Tag im Trainingsdatensatz ist der {0}, minus {1} Tage ergibt den {2}\".format(letztes_datum, diff.days, split_datum))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datensatz trennen\n",
        "train = train_sortiert.loc[train_sortiert.Datum <= split_datum]\n",
        "validation = train_sortiert.loc[train_sortiert.Datum > split_datum]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_len = len(train.index)\n",
        "validation_len = len(validation.index)\n",
        "\n",
        "print(\"Der Datensatz train_x_store hat {0} Zeilen und die Datensätze train und validation zusammen {1} Zeilen.\".format(len(train_x_store.index), train_len+validation_len))\n",
        "print(\"train: {0} Zeilen, validation: {1} Zeilen\".format(train_len, validation_len) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(15,5))\n",
        "train.plot(x=\"Datum\", y=\"Umsatz\", ax=ax, label=\"Train\")\n",
        "validation.plot(x=\"Datum\", y=\"Umsatz\", ax=ax, label=\"Validation\")\n",
        "ax.set_xlabel(\"Datum\")\n",
        "ax.set_ylabel(\"Umsatz\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJTuYUGp_17r"
      },
      "source": [
        "## Machine Learning Verfahren 1 - SARIMAX<a id=\"6\"></a> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notiz von Alice an Niklas: zum weiterarbeiten mit deinem ML-Verfahren:\n",
        "train_sarimax = train.copy()\n",
        "test_sarimax = validation.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SARIMAX steht für Seasonal Autoregressive Integrated Moving Average + exogenous variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(train_sarimax.head())\n",
        "display(test_sarimax.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zufällige Auswahl treffen\n",
        "sampled_values = []\n",
        "\n",
        "# Alle einzigartigen Werte in der Spalte Sortiment ermitteln\n",
        "unique_sortiments = train_sarimax['Sortiment'].unique()\n",
        "\n",
        "# Für jeden Wert im Sortiment eine zufällige Filiale auswählen\n",
        "for sortiment in unique_sortiments:\n",
        "    filialen = train_sarimax[train_sarimax['Sortiment'] == sortiment]['Filiale'].tolist()\n",
        "    sampled_values.append(random.choice(filialen))\n",
        "\n",
        "# Alle einzigartigen Werte in der Spalte Filialentyp ermitteln\n",
        "unique_filialentypen = train_sarimax['Filialentyp'].unique()\n",
        "\n",
        "# Für jeden Wert im Sortiment eine zufällige Filiale auswählen\n",
        "for filialentyp in unique_filialentypen:\n",
        "    filialen = train_sarimax[train_sarimax['Filialentyp'] == filialentyp]['Filiale'].tolist()\n",
        "    sampled_values.append(random.choice(filialen))\n",
        "\n",
        "# Zusätzliche Zufallswerte aus der Spalte Filiale auswählen, um insgesamt 20 Werte zu erhalten\n",
        "remaining_values = random.sample(train_sarimax['Filiale'].tolist(), 20 - len(sampled_values))\n",
        "sampled_values.extend(remaining_values)\n",
        "\n",
        "# # Eine Filiale zufällig wählen\n",
        "# store_ids = train_sarimax[\"Filiale\"].sample(n=10).values\n",
        "\n",
        "train_sample_stores = train_sarimax[train_sarimax[\"Filiale\"].isin(sampled_values)]\n",
        "train_sample_stores.reset_index(inplace=True)\n",
        "# train_sample_stores.set_index(\"Datum\", drop = True, inplace=True)\n",
        "test_sample_stores = test_sarimax[test_sarimax[\"Filiale\"].isin(sampled_values)]\n",
        "test_sample_stores.reset_index(inplace=True)\n",
        "# test_sample_stores.set_index(\"Datum\", drop = True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "exog_sarimax = pd.get_dummies(train_sample_stores[[\"Wochentag\", \"Filialentyp\", \"Sortiment\", \"Feiertag\", \"Datum\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "exog_sarimax = exog_sarimax.apply(pd.to_numeric, errors = \"coerce\")\n",
        "exog_sarimax = exog_sarimax.astype(\"int64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Auto-ARIMA Modell mit wöchentlicher Saisonalität\n",
        "sarimax_model = pm.auto_arima(train_sample_stores['Umsatz'],\n",
        "                           exogenous=exog_sarimax, # SARIMAX\n",
        "                           start_p=2, start_q=2,\n",
        "                           max_p=5, max_q=5,\n",
        "                           start_P=1,\n",
        "                           seasonal=True,  # SARIMA\n",
        "                           m=52,  # Wöchentliche Saisonalität (1: jährlich, 4: quartalweise, 52: wöchentlich)\n",
        "                           stepwise=True,  # Effiziente Suche\n",
        "                           trace=True)  # Zeige den Fortschritt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(5, 1, 1) (1, 0, 0) [52] hat am besten performt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sarimax_model)\n",
        "print(sarimax_model.order)\n",
        "print(sarimax_model.seasonal_order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "sarimax_model = SARIMAX(train_sample_stores[\"Umsatz\"],\n",
        "                        order = (2, 1, 2),# sarimax_model.order,\n",
        "                        seasonal_order= (1, 0, 0, 52), # sarimax_model.seasonal_order,\n",
        "                        exog = exog_sarimax,\n",
        "                        enforce_stationarity=True, \n",
        "                        enforce_invertibility=False)\n",
        "\n",
        "results_sarimax = sarimax_model.fit()\n",
        "print(results_sarimax.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# results_sarimax.save(\"sarimax\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# results_sarimax.plot_diagnostics(figsize=(15, 8))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eine Filiale zufällig wählen\n",
        "store_id = train_sarimax[\"Filiale\"].sample(n=1).values[0]\n",
        "\n",
        "train_sample_store = train_sarimax[train_sarimax[\"Filiale\"] == store_id]\n",
        "train_sample_store.set_index(\"Datum\", inplace = True, drop= True)\n",
        "test_sample_store = test_sarimax[test_sarimax[\"Filiale\"] == store_id][:56]\n",
        "# test_sample_store.set_index(\"Datum\", inplace = True, drop= True)\n",
        "test_sample_store.reset_index(inplace= True)\n",
        "\n",
        "# display(test_sample_store)\n",
        "\n",
        "test_exog_sarimax = pd.get_dummies(test_sample_store[[\"Wochentag\", \"Filialentyp\", \"Sortiment\", \"Feiertag\", \"Datum\"]])\n",
        "test_exog_sarimax = test_exog_sarimax.apply(pd.to_numeric, errors = \"coerce\")\n",
        "test_exog_sarimax = test_exog_sarimax.astype(\"int64\")\n",
        "\n",
        "# Finden der fehlenden Spalten in df1, die in df2 vorhanden sind\n",
        "missing_columns = [col for col in exog_sarimax.columns if col not in test_exog_sarimax.columns]\n",
        "\n",
        "# Hinzufügen der fehlenden Spalten zu df1 und initialisieren mit 0\n",
        "for col in missing_columns:\n",
        "    test_exog_sarimax[col] = 0\n",
        "# display(test_exog_sarimax)\n",
        "\n",
        "\n",
        "# Auszukommentieren beim Einsatz von statsmodel Sarimax anstatt auto_arima\n",
        "forecast = results_sarimax.get_forecast(steps=len(test_exog_sarimax), exog=test_exog_sarimax)\n",
        "forecast_mean = forecast.predicted_mean\n",
        "\n",
        "# display(forecast.predicted_mean)\n",
        "\n",
        "# predictions = sarimax_model.predict(n_periods=len(test_sample_store), exogenous=test_exog_sarimax)\n",
        "# print(predictions)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(train_sample_store.index[-30:], train_sample_store['Umsatz'][-30:], label='Historischer Umsatz')\n",
        "plt.plot(test_sample_store[\"Datum\"], test_sample_store['Umsatz'], label='tatsächlicher Umsatz', linestyle = \":\")\n",
        "plt.plot(test_sample_store[\"Datum\"], forecast_mean, label='Prognostizierter Umsatz', linestyle='--')\n",
        "plt.title('Umsatzprognose mittels SARIMAX')\n",
        "plt.xlabel('Datum')\n",
        "plt.ylabel('Umsatz')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIOrDbWz_9vt"
      },
      "source": [
        "## Machine Learning Verfahren 2 - Prophet <a id=\"7\"></a> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prophet Dokumentation: https://facebook.github.io/prophet/docs/quick_start.html\n",
        "\n",
        "Prophet follows the sklearn model API. We create an instance of the Prophet class and then call its fit and predict methods. The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date. The y column must be numeric, and represents the measurement we wish to forecast.\n",
        "\n",
        "\"The best way to handle outliers is to remove them - Prophet has no problem with missing data. If you set their values to NA in the history but leave the dates in future, then Prophet will give you a prediction for their values.\"\n",
        "\n",
        "Wir werden die nächsten X Tage je Filientyp vorhersagen, dafür verwenden wir den Traindatensatz.\n",
        "\n",
        "--\n",
        "\n",
        "Um selbst Feiertage hinzuzufügen, benötigen wir einen separaten Datensatz mit den Spalten \"Feiertag\" und \"ds\". In dem Datensatz müssen alle vergangenen und zukünftigen Feiertage für die Vorhersage vorhanden sein. Prophet bietet aber auch bereits vordefinierte gesetzliche Feiertage, welche unter https://github.com/vacanza/python-holidays/ zu finden sind. Für Deutschland sind dort folgende Feiertage enthalten:\n",
        "\n",
        "- Neujahr\n",
        "- Karfreitag\n",
        "- Ostermontag\n",
        "- Tag der Arbeit\n",
        "- Christi Himmelfahrt\n",
        "- Pfingstmontag\n",
        "- Tag der deutschen Einheit\n",
        "- Erster Weihnachtstag\n",
        "- Zweiter Weihnachtstag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 878,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_prophet = train\n",
        "validation_prophet = validation\n",
        "performance_vergleichen = pd.DataFrame(data=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wir erstellen vorab ein Dataframe mit unseren Vorhersagen, damit können wir im letzten Kapitel die Performance unserer beider Machine Learning Verfahren vergleichen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 879,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vorhersage_prophet(filialennr, anzahl_vorhersagetage, output_anzeigen):\n",
        "\n",
        "    train_df = train_prophet[train_prophet.Filiale==filialennr][[\"Datum\", \"Umsatz\"]]        # Datensatz nach Filiale filtern\n",
        "    train_df = train_df.sort_values(by=[\"Datum\"], ascending=True).reset_index(drop = True)  # Datensatz aufsteigend sortieren\n",
        "    train_df = train_df.rename(columns = {\"Datum\": \"ds\", \"Umsatz\": \"y\"})                    # Spalten umbennenen\n",
        "\n",
        "    model = Prophet(interval_width = 0.95)                                      # Modell mit 95% Konfidenzintervall erstellen\n",
        "    model.add_country_holidays(country_name='DE')                               # Feiertage in Deutschland hinzufügen\n",
        "    model.fit(df)                                                               # Modell mit den Daten anreichern                                                              \n",
        "    future = model.make_future_dataframe(periods=anzahl_vorhersagetage)         # Tage die vorhergesagt werden sollen hinzufügen\n",
        "    vorhersage = model.predict(future)                                          # Vorhersagen erstellen\n",
        "    vorhersage_df = vorhersage[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]      # Vorhersagedatensatz speichern\n",
        "\n",
        "    # Output\n",
        "    if output_anzeigen==True:\n",
        "        display(vorhersage_df)\n",
        "        figure1 = model.plot(vorhersage, xlabel = 'Data', ylabel = 'Vendas')\n",
        "        figure2 = model.plot_components(vorhersage)\n",
        "\n",
        "    #train_x_store[train_x_store.Filiale==filialennr][[\"Datum\", \"Umsatz\"]]\n",
        "\n",
        "    # Umsätze und Vorhersagen mergen\n",
        "    performance_merge = vorhersage_df[[\"ds\", \"yhat\"]].merge(train_x_store[train_x_store.Filiale==filialennr][[\"Datum\", \"Umsatz\"]],\n",
        "                                                            left_on='ds', right_on='Datum', how=\"left\")\n",
        "\n",
        "    # Performance zum vergleichen ablegen\n",
        "    performance_vergleich = pd.DataFrame(data = {\"Datum\": performance_merge[\"ds\"],\n",
        "                                                 \"Filiale\": filialennr,\n",
        "                                                 \"y_true\": performance_merge[\"Umsatz\"],\n",
        "                                                 \"y_pred_alice\": performance_merge[\"yhat\"]\n",
        "                                                 })\n",
        "    # RSMPE Performance messen und ablegen\n",
        "    # wurzel [ ((yi-yhat)/yi )^2 ]\n",
        "    performance_vergleich[\"rsmpe_alice\"] = np.sqrt(np.square(((performance_vergleich[\"y_true\"] - performance_vergleich[\"y_pred_alice\"])/performance_vergleich[\"y_true\"])))\n",
        "\n",
        "    \n",
        "    return vorhersage, performance_vergleich"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(1, 201):\n",
        "    vorhersage, performance_vergleich = vorhersage_prophet(filialennr=i, anzahl_vorhersagetage=48, output_anzeigen=False)\n",
        "    performance_vergleichen = pd.concat([performance_vergleichen, performance_vergleich], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "performance_vergleichen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance messen mit RSMPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the score as per the definition on https://www.kaggle.com/c/rossmann-store-sales#evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rmspe(y_true, y_pred):\n",
        "    performance_rmspe = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n",
        "    return performance_rmspe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHn8zP2f__P0"
      },
      "source": [
        "## Vergleich der Machine Learning Verfahren <a id=\"8\"></a> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_store_ids = list(range(1, 201))\n",
        "\n",
        "\n",
        "validation_sample = validation[validation[\"Filiale\"].isin(sample_store_ids)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sarimax_predictions = pd.DataFrame(columns= [\"Datum\", \"Filiale\", \"y_true\", \"y_pred\", \"rmspe\"])\n",
        "gp_validation_sample = validation_sample.groupby(by = \"Filiale\")\n",
        "\n",
        "for store_id, store_validation_data in gp_validation_sample:\n",
        "    # print(store_validation_data)\n",
        "    validation_exog = pd.get_dummies(store_validation_data[[\"Wochentag\", \"Filialentyp\", \"Sortiment\", \"Feiertag\", \"Datum\"]])\n",
        "    validation_exog = validation_exog.apply(pd.to_numeric, errors = \"coerce\")\n",
        "    validation_exog = validation_exog.astype(\"int64\")\n",
        "\n",
        "    # Finden der fehlenden Spalten in df1, die in df2 vorhanden sind\n",
        "    missing_columns = [col for col in exog_sarimax.columns if col not in validation_exog.columns]\n",
        "\n",
        "    # Hinzufügen der fehlenden Spalten zu df1 und initialisieren mit 0\n",
        "    for col in missing_columns:\n",
        "        validation_exog[col] = 0\n",
        "    \n",
        "    forecast = results_sarimax.get_forecast(steps=len(validation_exog), exog=validation_exog)\n",
        "    forecast_mean = forecast.predicted_mean \n",
        "    # print(type(forecast_mean))\n",
        "\n",
        "    store_results = pd.DataFrame(columns= [\"Datum\", \"Filiale\", \"y_true\", \"y_pred\", \"rmspe\"])\n",
        "\n",
        "    store_results[\"Datum\"] = store_validation_data[\"Datum\"]\n",
        "    store_results[\"Filiale\"] = store_validation_data[\"Filiale\"]\n",
        "    store_results[\"y_true\"] = store_validation_data[\"Umsatz\"]\n",
        "    store_results[\"y_pred\"] = forecast_mean.values\n",
        "    store_results[\"rmspe\"] = np.sqrt(np.square(((store_results[\"y_true\"] - store_results[\"y_pred\"])/store_results[\"y_true\"])))\n",
        "\n",
        "\n",
        "    sarimax_predictions = pd.concat([sarimax_predictions, store_results])\n",
        "\n",
        "display(sarimax_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "sarimax_predictions.to_csv(\"sarimax_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMYuAFrHlEVIBdFp5vtNX+s",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
